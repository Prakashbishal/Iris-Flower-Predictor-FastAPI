
# ğŸŒ¸ Iris Flower Prediction API (FastAPI)

A **production-ready, modular FastAPI application** that predicts the species of an Iris flower based on physical measurements and enriches the response with a representative flower image fetched from Wikipedia.

This project demonstrates **clean backend architecture for ML inference**, following industry-standard separation of concerns and validation practices.

---

## âœ¨ Key Features

* ğŸ§  Machine Learning inference using a **scikit-learn pipeline**
* ğŸ“ Accepts **real-world measurements (cm)** with strict validation
* ğŸ–¼ï¸ Automatically fetches a representative flower image from **Wikipedia**
* ğŸ“Š Optional class probabilities
* ğŸ§± Clean modular architecture (routes, schemas, services, config)
* ğŸ§ª Response validation using **Pydantic response models**
* ğŸ³ Designed for Docker & cloud deployment
* â¤ï¸ Health check endpoint for monitoring

---

## ğŸ§  Machine Learning Model

* **Dataset**: Iris dataset (scikit-learn)
* **Features**:

  * Sepal length (cm)
  * Sepal width (cm)
  * Petal length (cm)
  * Petal width (cm)
* **Pipeline**:

  * `StandardScaler`
  * `LogisticRegression`
* **Classes**:

  | ID | Class      |
  | -- | ---------- |
  | 0  | setosa     |
  | 1  | versicolor |
  | 2  | virginica  |

The model is trained on raw centimeter values and saved as a **single pipeline**, ensuring consistent preprocessing during inference.

---

## ğŸ“ Project Structure

```
Iris_Flower_Prediction/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # App factory & router wiring
â”‚   â”œâ”€â”€ config.py            # Environment-based configuration
â”‚   â”œâ”€â”€ schemas.py           # Pydantic request & response models
â”‚   â”œâ”€â”€ model_loader.py      # ML model loading logic
â”‚   â”‚
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ predict.py       # /predict endpoint
â”‚   â”‚   â””â”€â”€ health.py        # /health endpoint
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ predict.py       # ML prediction logic
â”‚   â”‚   â””â”€â”€ wiki.py          # Wikipedia image fetching
â”‚   â”‚
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ model/
â”‚   â””â”€â”€ saved_model_iris.pkl # Trained ML pipeline
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”— API Endpoints

### ğŸ”® `POST /predict`

Predict the Iris flower species.

#### Request Body

```json
{
  "sepal_length": 5.1,
  "sepal_width": 3.5,
  "petal_length": 1.4,
  "petal_width": 0.2
}
```

#### Input Validation (cm)

| Feature      | Min | Max |
| ------------ | --- | --- |
| Sepal length | 4.3 | 7.9 |
| Sepal width  | 2.0 | 4.4 |
| Petal length | 1.0 | 6.9 |
| Petal width  | 0.1 | 2.5 |

Invalid inputs automatically return **422 Unprocessable Entity** with detailed error messages.

---

#### Response

```json
{
  "predicted_category_id": 0,
  "predicted_category_name": "setosa",
  "image_url": "https://upload.wikimedia.org/...",
  "probabilities": {
    "setosa": 0.98,
    "versicolor": 0.01,
    "virginica": 0.01
  }
}
```

---

### â¤ï¸ `GET /health`

Health check endpoint for Docker/Kubernetes.

```json
{
  "status": "ok",
  "model_loaded": true
}
```

---

## ğŸ§¾ Response Validation

The API uses **Pydantic response models** to validate output structure.

* Ensures correct types & keys
* Automatically documented in `/docs`
* Prevents accidental malformed responses

Validation is applied via:

```python
@router.post("/predict", response_model=PredictResponse)
```

---

## âš™ï¸ Configuration (Environment Variables)

| Variable           | Description                     | Default                        |
| ------------------ | ------------------------------- | ------------------------------ |
| `MODEL_PATH`       | Path to ML model                | `./model/saved_model_iris.pkl` |
| `ENABLE_WIKI`      | Enable Wikipedia image fetching | `true`                         |
| `WIKI_TIMEOUT_SEC` | Wikipedia API timeout (sec)     | `6`                            |
| `WIKI_USER_AGENT`  | HTTP user agent                 | `iris-fastapi/1.0`             |
| `LOG_LEVEL`        | Logging level                   | `INFO`                         |

---

## â–¶ï¸ Running Locally

### 1ï¸âƒ£ Create virtual environment

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

### 2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run the API

```bash
uvicorn app.main:app --reload
```

### 4ï¸âƒ£ Open Swagger UI

```
http://127.0.0.1:8000/docs
```

---

## ğŸ³ Docker Ready

The app is designed for containerization:

* Model loaded at startup
* Stateless requests
* Configurable via environment variables
* Health endpoint for orchestration

*(Dockerfile can be added later without refactoring code)*

---

## ğŸ›¡ï¸ Production Considerations

* Model loaded once at startup (fast inference)
* External API calls use retries & caching
* Input validation at API boundary
* Response validation enforced
* Graceful degradation if Wikipedia fails

---

## ğŸ”® Possible Enhancements

* Dockerfile & CI pipeline
* Rate limiting & auth
* Structured logging & metrics
* Model versioning
* Multiple ML models
* Image source fallback

---

## ğŸ‘¤ Author

**Bishal Pandey**
<br>
MSc Artificial Intelligence
University of Southampton

---
